{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXtZ1eA64T8C"
      },
      "source": [
        "# Reproduction notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pylRm3wVF38"
      },
      "source": [
        "\n",
        "\n",
        "<\"\"\" \"\"\"> and <#> : comments added by authors.\n",
        "\n",
        "markdown block and <###> : description and comments added by FSFM MADS DEEP LEARNING student group.\n",
        "\n",
        "No additions to the original code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo8vCQIqUgL2"
      },
      "source": [
        "# Package import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwiEdWgQUgL3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt2ChMf0UgL5"
      },
      "outputs": [],
      "source": [
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guPH_2UTUgL5"
      },
      "outputs": [],
      "source": [
        "from pandas.tseries.holiday import EasterMonday\n",
        "from pandas.tseries.holiday import GoodFriday\n",
        "from pandas.tseries.holiday import Holiday\n",
        "from pandas.tseries.holiday import SU\n",
        "from pandas.tseries.holiday import TH\n",
        "from pandas.tseries.holiday import USColumbusDay\n",
        "from pandas.tseries.holiday import USLaborDay\n",
        "from pandas.tseries.holiday import USMartinLutherKingJr\n",
        "from pandas.tseries.holiday import USMemorialDay\n",
        "from pandas.tseries.holiday import USPresidentsDay\n",
        "from pandas.tseries.holiday import USThanksgivingDay\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "from pandas.tseries.offsets import Day\n",
        "from pandas.tseries.offsets import Easter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EipcAjVmUgL5"
      },
      "source": [
        "# Time features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSPsUORETb1_"
      },
      "source": [
        "This block is used to extract and contruct time features listed below:\n",
        "\n",
        "1. moh: minute_of_hour\n",
        "2. hod: hour_of_day\n",
        "3. dom: day_of_month\n",
        "4. dow: day_of_week\n",
        "5. doy: day_of_year\n",
        "6. moy: month_of_year\n",
        "7. woy: week_of_year\n",
        "8. normalized distance (days) between the timestamp of a sample and each holiday (18 holidays are taken into consideration, so 18 columns will be generated)\n",
        "\n",
        "Input is datetime index of dataset and holiday.\n",
        "\n",
        "Output is a dataframe that contains features mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELShnXbLUgL6"
      },
      "outputs": [],
      "source": [
        "\"\"\"Directory to extract time covariates.\n",
        "\n",
        "Extract time covariates from datetime.\n",
        "\"\"\"\n",
        "\n",
        "# This is 183 to cover half a year (in both directions), also for leap years\n",
        "# + 17 as Eastern can be between March, 22 - April, 25\n",
        "MAX_WINDOW = 183 + 17\n",
        "\n",
        "\n",
        "def _distance_to_holiday(holiday):\n",
        "  \"\"\"Return distance to given holiday.\"\"\"\n",
        "\n",
        "  def _distance_to_day(index):\n",
        "    holiday_date = holiday.dates(\n",
        "        index - pd.Timedelta(days=MAX_WINDOW),\n",
        "        index + pd.Timedelta(days=MAX_WINDOW),\n",
        "    )\n",
        "    assert (\n",
        "        len(holiday_date) != 0  # pylint: disable=g-explicit-length-test\n",
        "    ), f\"No closest holiday for the date index {index} found.\"\n",
        "    # It sometimes returns two dates if it is exactly half a year after the\n",
        "    # holiday. In this case, the smaller distance (182 days) is returned.\n",
        "    return (index - holiday_date[0]).days\n",
        "\n",
        "  return _distance_to_day\n",
        "\n",
        "\n",
        "EasterSunday = Holiday(\n",
        "    \"Easter Sunday\", month=1, day=1, offset=[Easter(), Day(0)]\n",
        ")\n",
        "NewYearsDay = Holiday(\"New Years Day\", month=1, day=1)\n",
        "SuperBowl = Holiday(\n",
        "    \"Superbowl\", month=2, day=1, offset=DateOffset(weekday=SU(1))\n",
        ")\n",
        "MothersDay = Holiday(\n",
        "    \"Mothers Day\", month=5, day=1, offset=DateOffset(weekday=SU(2))\n",
        ")\n",
        "IndependenceDay = Holiday(\"Independence Day\", month=7, day=4)\n",
        "ChristmasEve = Holiday(\"Christmas\", month=12, day=24)\n",
        "ChristmasDay = Holiday(\"Christmas\", month=12, day=25)\n",
        "NewYearsEve = Holiday(\"New Years Eve\", month=12, day=31)\n",
        "BlackFriday = Holiday(\n",
        "    \"Black Friday\",\n",
        "    month=11,\n",
        "    day=1,\n",
        "    offset=[pd.DateOffset(weekday=TH(4)), Day(1)],\n",
        ")\n",
        "CyberMonday = Holiday(\n",
        "    \"Cyber Monday\",\n",
        "    month=11,\n",
        "    day=1,\n",
        "    offset=[pd.DateOffset(weekday=TH(4)), Day(4)],\n",
        ")\n",
        "\n",
        "HOLIDAYS = [\n",
        "    EasterMonday,\n",
        "    GoodFriday,\n",
        "    USColumbusDay,\n",
        "    USLaborDay,\n",
        "    USMartinLutherKingJr,\n",
        "    USMemorialDay,\n",
        "    USPresidentsDay,\n",
        "    USThanksgivingDay,\n",
        "    EasterSunday,\n",
        "    NewYearsDay,\n",
        "    SuperBowl,\n",
        "    MothersDay,\n",
        "    IndependenceDay,\n",
        "    ChristmasEve,\n",
        "    ChristmasDay,\n",
        "    NewYearsEve,\n",
        "    BlackFriday,\n",
        "    CyberMonday,\n",
        "]\n",
        "\n",
        "\n",
        "class TimeCovariates(object):\n",
        "  \"\"\"Extract all time covariates except for holidays.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      datetimes,\n",
        "      normalized = True,\n",
        "      holiday = False,\n",
        "  ):\n",
        "    \"\"\"Init function.\n",
        "\n",
        "    Args:\n",
        "      datetimes: pandas DatetimeIndex (lowest granularity supported is min)\n",
        "      normalized: whether to normalize features or not\n",
        "      holiday: fetch holiday features or not\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    self.normalized = normalized\n",
        "    self.dti = datetimes\n",
        "    self.holiday = holiday\n",
        "\n",
        "  def _minute_of_hour(self):\n",
        "    minutes = np.array(self.dti.minute, dtype=np.float32)\n",
        "    if self.normalized:\n",
        "      minutes = minutes / 59.0 - 0.5\n",
        "    return minutes\n",
        "\n",
        "  def _hour_of_day(self):\n",
        "    hours = np.array(self.dti.hour, dtype=np.float32)\n",
        "    if self.normalized:\n",
        "      hours = hours / 23.0 - 0.5\n",
        "    return hours\n",
        "\n",
        "  def _day_of_week(self):\n",
        "    day_week = np.array(self.dti.dayofweek, dtype=np.float32)\n",
        "    if self.normalized:\n",
        "      day_week = day_week / 6.0 - 0.5\n",
        "    return day_week\n",
        "\n",
        "  def _day_of_month(self):\n",
        "    day_month = np.array(self.dti.day, dtype=np.float32)\n",
        "    if self.normalized:\n",
        "      day_month = day_month / 30.0 - 0.5\n",
        "    return day_month\n",
        "\n",
        "  def _day_of_year(self):\n",
        "    day_year = np.array(self.dti.dayofyear, dtype=np.float32)\n",
        "    if self.normalized:\n",
        "      day_year = day_year / 364.0 - 0.5\n",
        "    return day_year\n",
        "\n",
        "  def _month_of_year(self):\n",
        "    month_year = np.array(self.dti.month, dtype=np.float32)\n",
        "    if self.normalized:\n",
        "      month_year = month_year / 11.0 - 0.5\n",
        "    return month_year\n",
        "\n",
        "  def _week_of_year(self):\n",
        "    week_year = np.array(self.dti.strftime(\"%U\").astype(int), dtype=np.float32)\n",
        "    if self.normalized:\n",
        "      week_year = week_year / 51.0 - 0.5\n",
        "    return week_year\n",
        "\n",
        "  def _get_holidays(self):\n",
        "    dti_series = self.dti.to_series()\n",
        "    hol_variates = np.vstack(\n",
        "        [\n",
        "            dti_series.apply(_distance_to_holiday(h)).values\n",
        "            for h in tqdm(HOLIDAYS)\n",
        "        ]\n",
        "    )\n",
        "    # hol_variates is (num_holiday, num_time_steps), the normalization should be\n",
        "    # performed in the num_time_steps dimension.\n",
        "    return StandardScaler().fit_transform(hol_variates.T).T\n",
        "\n",
        "  def get_covariates(self):\n",
        "    \"\"\"Get all time covariates.\"\"\"\n",
        "    moh = self._minute_of_hour().reshape(1, -1)\n",
        "    hod = self._hour_of_day().reshape(1, -1)\n",
        "    dom = self._day_of_month().reshape(1, -1)\n",
        "    dow = self._day_of_week().reshape(1, -1)\n",
        "    doy = self._day_of_year().reshape(1, -1)\n",
        "    moy = self._month_of_year().reshape(1, -1)\n",
        "    woy = self._week_of_year().reshape(1, -1)\n",
        "\n",
        "    all_covs = [\n",
        "        moh,\n",
        "        hod,\n",
        "        dom,\n",
        "        dow,\n",
        "        doy,\n",
        "        moy,\n",
        "        woy,\n",
        "    ]\n",
        "    columns = [\"moh\", \"hod\", \"dom\", \"dow\", \"doy\", \"moy\", \"woy\"]\n",
        "    if self.holiday:\n",
        "      hol_covs = self._get_holidays()\n",
        "      all_covs.append(hol_covs)\n",
        "      columns += [f\"hol_{i}\" for i in range(len(HOLIDAYS))]\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        data=np.vstack(all_covs).transpose(),\n",
        "        columns=columns,\n",
        "        index=self.dti,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ioGR8xGUgL6"
      },
      "source": [
        "# Data loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjPXz7Qj4lmf"
      },
      "source": [
        "this block is used to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txdaYd4AUgL7"
      },
      "outputs": [],
      "source": [
        "\"\"\"TF dataloaders for general timeseries datasets.\n",
        "\n",
        "The expected input format is csv file with a datetime index.\n",
        "\"\"\"\n",
        "\n",
        "class TimeSeriesdata(object):\n",
        "  \"\"\"Data loader class.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      data_path,\n",
        "      datetime_col,\n",
        "      num_cov_cols,\n",
        "      cat_cov_cols,\n",
        "      ts_cols,\n",
        "      train_range,\n",
        "      val_range,\n",
        "      test_range,\n",
        "      hist_len,\n",
        "      pred_len,\n",
        "      batch_size,\n",
        "      freq='H',\n",
        "      normalize=True,\n",
        "      epoch_len=None,\n",
        "      holiday=False,\n",
        "      permute=True,\n",
        "  ):\n",
        "    \"\"\"Initialize objects.\n",
        "\n",
        "    Args:\n",
        "      data_path: path to csv file\n",
        "      datetime_col: column name for datetime col\n",
        "      num_cov_cols: list of numerical global covariates\n",
        "      cat_cov_cols: list of categorical global covariates\n",
        "      ts_cols: columns corresponding to ts\n",
        "      train_range: tuple of train ranges\n",
        "      val_range: tuple of validation ranges\n",
        "      test_range: tuple of test ranges\n",
        "      hist_len: historical context\n",
        "      pred_len: prediction length\n",
        "      batch_size: batch size (number of ts in a batch)\n",
        "      freq: freq of original data\n",
        "      normalize: std. normalize data or not\n",
        "      epoch_len: num iters in an epoch\n",
        "      holiday: use holiday features or not\n",
        "      permute: permute ts in train batches or not\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    self.data_df = pd.read_csv(open(data_path, 'r'))\n",
        "    if not num_cov_cols:\n",
        "      self.data_df['ncol'] = np.zeros(self.data_df.shape[0])\n",
        "      num_cov_cols = ['ncol']\n",
        "    if not cat_cov_cols:\n",
        "      self.data_df['ccol'] = np.zeros(self.data_df.shape[0])\n",
        "      cat_cov_cols = ['ccol']\n",
        "    self.data_df.fillna(0, inplace=True)\n",
        "    self.data_df.set_index(\n",
        "        pd.DatetimeIndex(self.data_df[datetime_col]), inplace=True\n",
        "    )\n",
        "    self.num_cov_cols = num_cov_cols\n",
        "    self.cat_cov_cols = cat_cov_cols\n",
        "    self.ts_cols = ts_cols\n",
        "    self.train_range = train_range\n",
        "    self.val_range = val_range\n",
        "    self.test_range = test_range\n",
        "    data_df_idx = self.data_df.index\n",
        "\n",
        "    ### date_index includes:\n",
        "    ### 1. the time point in the original data index of dataset, and\n",
        "    ### 2. pred_len + 1 time points in the future, which have the same frequency.\n",
        "    date_index = data_df_idx.union(\n",
        "        pd.date_range(\n",
        "            data_df_idx[-1] + pd.Timedelta(1, freq=freq),\n",
        "            periods=pred_len + 1,\n",
        "            freq=freq,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    ### create a time_df that contains time covariates for all time points in date_index\n",
        "    self.time_df = TimeCovariates(\n",
        "        date_index, holiday=holiday\n",
        "    ).get_covariates()\n",
        "\n",
        "    self.hist_len = hist_len\n",
        "    self.pred_len = pred_len\n",
        "    self.batch_size = batch_size\n",
        "    self.freq = freq\n",
        "    self.normalize = normalize\n",
        "\n",
        "    ### create numpy matrices forï¼š\n",
        "    ### 1. time series\n",
        "    ### 2. time covariates\n",
        "    ### 3. numerical global covariates\n",
        "    ### 4. categorical global covariates\n",
        "    self.data_mat = self.data_df[self.ts_cols].to_numpy().transpose()\n",
        "    self.data_mat = self.data_mat[:, 0 : self.test_range[1]]\n",
        "    self.time_mat = self.time_df.to_numpy().transpose()\n",
        "    self.num_feat_mat = self.data_df[num_cov_cols].to_numpy().transpose()\n",
        "    self.cat_feat_mat, self.cat_sizes = self._get_cat_cols(cat_cov_cols)\n",
        "\n",
        "    self.normalize = normalize\n",
        "    if normalize:\n",
        "      self._normalize_data()\n",
        "    logging.info(\n",
        "        'Data Shapes: %s, %s, %s, %s',\n",
        "        self.data_mat.shape,\n",
        "        self.time_mat.shape,\n",
        "        self.num_feat_mat.shape,\n",
        "        self.cat_feat_mat.shape,\n",
        "    )\n",
        "    self.epoch_len = epoch_len\n",
        "    self.permute = permute\n",
        "\n",
        "### if a dataset contains categorical covaraites, for each covariate column:\n",
        "### dct: a dictionary assign an index to each unique categorical value\n",
        "### cat_sizes: the number of unique categorical value\n",
        "### mapped: an array stating the index of the categorical value of each sample\n",
        "###   for example, a dataset involves categorical covariate \"location\"\n",
        "###   location columns is ['Newyork','Paris','London','Paris','Newyork']\n",
        "###   dct = {'Newyork':0, 'Paris':1,'London':2}\n",
        "###   cat_sizes = 3\n",
        "###   mapped -> array([0, 1, 2, 1, 0])\n",
        "### output contains:\n",
        "### 1.  np.vstack(cat_vars): a matrix contains mapped for all categorical covariates in a df\n",
        "### 2.  cat_sizes: a list of cat_size of all categorical covariates\n",
        "  def _get_cat_cols(self, cat_cov_cols):\n",
        "    \"\"\"Get categorical columns.\"\"\"\n",
        "    cat_vars = []\n",
        "    cat_sizes = []\n",
        "    for col in cat_cov_cols:\n",
        "      dct = {x: i for i, x in enumerate(self.data_df[col].unique())}\n",
        "      cat_sizes.append(len(dct))\n",
        "      mapped = self.data_df[col].map(lambda x: dct[x]).to_numpy().transpose()  # pylint: disable=cell-var-from-loop\n",
        "      cat_vars.append(mapped)\n",
        "    return np.vstack(cat_vars), cat_sizes\n",
        "\n",
        "  def _normalize_data(self):\n",
        "    self.scaler = StandardScaler()\n",
        "    train_mat = self.data_mat[:, self.train_range[0] : self.train_range[1]]\n",
        "    self.scaler = self.scaler.fit(train_mat.transpose())\n",
        "    self.data_mat = self.scaler.transform(self.data_mat.transpose()).transpose()\n",
        "\n",
        "\n",
        "  def train_gen(self):\n",
        "    \"\"\"Generator for training data.\"\"\"\n",
        "    num_ts = len(self.ts_cols)\n",
        "\n",
        "    ### perm contains a series of index values\n",
        "    ### that are used to select the time series to include in the training phase.\n",
        "    ### permutation is applied to introduce randomness into the training process.\n",
        "    perm = np.arange(\n",
        "        self.train_range[0] + self.hist_len,\n",
        "        self.train_range[1] - self.pred_len,\n",
        "    )\n",
        "    perm = np.random.permutation(perm)\n",
        "\n",
        "    hist_len = self.hist_len\n",
        "    logging.info('Hist len: %s', hist_len)\n",
        "\n",
        "    if not self.epoch_len:\n",
        "      epoch_len = len(perm)\n",
        "    else:\n",
        "      epoch_len = self.epoch_len\n",
        "    for idx in perm[0:epoch_len]:\n",
        "      for _ in range(num_ts // self.batch_size + 1):\n",
        "        if self.permute:\n",
        "          tsidx = np.random.choice(num_ts, size=self.batch_size, replace=False)\n",
        "        else:\n",
        "          tsidx = np.arange(num_ts)\n",
        "        ### If self.permute is True, self.batch_size unique time series indexes are randomly selected.\n",
        "        ### in each training batch, a different time series is selected for training\n",
        "        ### This is useful for introducing randomness and diversity into the model during training.\n",
        "        ### If self.permute is False, all time series indexes will be selected,\n",
        "        ### all time series will be used for training\n",
        "        dtimes = np.arange(idx - hist_len, idx + self.pred_len)\n",
        "        (\n",
        "            bts_train,\n",
        "            bts_pred,\n",
        "            bfeats_train,\n",
        "            bfeats_pred,\n",
        "            bcf_train,\n",
        "            bcf_pred,\n",
        "        ) = self._get_features_and_ts(dtimes, tsidx, hist_len)\n",
        "\n",
        "        all_data = [\n",
        "            bts_train,\n",
        "            bfeats_train,\n",
        "            bcf_train,\n",
        "            bts_pred,\n",
        "            bfeats_pred,\n",
        "            bcf_pred,\n",
        "            tsidx,\n",
        "        ]\n",
        "        yield tuple(all_data)\n",
        "\n",
        "  def test_val_gen(self, mode='val'):\n",
        "    \"\"\"Generator for validation/test data.\"\"\"\n",
        "    if mode == 'val':\n",
        "      start = self.val_range[0]\n",
        "      end = self.val_range[1] - self.pred_len + 1\n",
        "    elif mode == 'test':\n",
        "      start = self.test_range[0]\n",
        "      end = self.test_range[1] - self.pred_len + 1\n",
        "    else:\n",
        "      raise NotImplementedError('Eval mode not implemented')\n",
        "    num_ts = len(self.ts_cols)\n",
        "    hist_len = self.hist_len\n",
        "    logging.info('Hist len: %s', hist_len)\n",
        "    perm = np.arange(start, end)\n",
        "    if self.epoch_len:\n",
        "      epoch_len = self.epoch_len\n",
        "    else:\n",
        "      epoch_len = len(perm)\n",
        "    for idx in perm[0:epoch_len]:\n",
        "      for batch_idx in range(0, num_ts, self.batch_size):\n",
        "        tsidx = np.arange(batch_idx, min(batch_idx + self.batch_size, num_ts))\n",
        "        dtimes = np.arange(idx - hist_len, idx + self.pred_len)\n",
        "        (\n",
        "            bts_train,\n",
        "            bts_pred,\n",
        "            bfeats_train,\n",
        "            bfeats_pred,\n",
        "            bcf_train,\n",
        "            bcf_pred,\n",
        "        ) = self._get_features_and_ts(dtimes, tsidx, hist_len)\n",
        "        all_data = [\n",
        "            bts_train,\n",
        "            bfeats_train,\n",
        "            bcf_train,\n",
        "            bts_pred,\n",
        "            bfeats_pred,\n",
        "            bcf_pred,\n",
        "            tsidx,\n",
        "        ]\n",
        "        yield tuple(all_data)\n",
        "\n",
        "### dtimes: np array of timestamp idx generated in train_gen and test_val_gen, indicating the specified window\n",
        "###         from (current timestamp - hist_len) to (current timestamp + pred_len)\n",
        "### tsidx: elements represent the position of the selected time series in the entire time series collection\n",
        "  def _get_features_and_ts(self, dtimes, tsidx, hist_len=None):\n",
        "    \"\"\"Get features and ts in specified windows.\"\"\"\n",
        "    if hist_len is None:\n",
        "      hist_len = self.hist_len\n",
        "    data_times = dtimes[dtimes < self.data_mat.shape[1]]\n",
        "    ### data_times: intersection of dtimes and timestamp index of the dataset, the available specified window\n",
        "    ### eg. when current timestamp = the last timestamp of dataset part of the dtimes is out of range\n",
        "    bdata = self.data_mat[:, data_times] ### time series matrix in specified window\n",
        "    bts = bdata[tsidx, :] ### time series selected for a training batch\n",
        "    bnf = self.num_feat_mat[:, data_times] ### numerical covariate matrix in specified window\n",
        "    bcf = self.cat_feat_mat[:, data_times] ### categorical covariate matrix in specified window\n",
        "    btf = self.time_mat[:, dtimes] ### time covariate matrix in specified window + prediction horizon\n",
        "    if bnf.shape[1] < btf.shape[1]: ### time covariate matrix covers longer time steps, deal with the diffence in shape\n",
        "      rem_len = btf.shape[1] - bnf.shape[1]\n",
        "      rem_rep = np.repeat(bnf[:, [-1]], repeats=rem_len)\n",
        "      rem_rep_cat = np.repeat(bcf[:, [-1]], repeats=rem_len)\n",
        "      bnf = np.hstack([bnf, rem_rep.reshape(bnf.shape[0], -1)])\n",
        "      bcf = np.hstack([bcf, rem_rep_cat.reshape(bcf.shape[0], -1)])\n",
        "    bfeats = np.vstack([btf, bnf]) ### time covariate + numerical covariate -> feature matrix\n",
        "    bts_train = bts[:, 0:hist_len]\n",
        "    bts_pred = bts[:, hist_len:]\n",
        "    bfeats_train = bfeats[:, 0:hist_len]\n",
        "    bfeats_pred = bfeats[:, hist_len:]\n",
        "    bcf_train = bcf[:, 0:hist_len]\n",
        "    bcf_pred = bcf[:, hist_len:]\n",
        "    return bts_train, bts_pred, bfeats_train, bfeats_pred, bcf_train, bcf_pred\n",
        "\n",
        "### function for train, validation and test datasets generation\n",
        "  def tf_dataset(self, mode='train'):\n",
        "    \"\"\"Tensorflow Dataset.\"\"\"\n",
        "    if mode == 'train':\n",
        "      gen_fn = self.train_gen\n",
        "    else:\n",
        "      gen_fn = lambda: self.test_val_gen(mode)\n",
        "    output_types = tuple(\n",
        "        [tf.float32] * 2 + [tf.int32] + [tf.float32] * 2 + [tf.int32] * 2\n",
        "    )\n",
        "    dataset = tf.data.Dataset.from_generator(gen_fn, output_types)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au1763iyUgL7"
      },
      "source": [
        "# TiDE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8HVvim_UgL8"
      },
      "outputs": [],
      "source": [
        "EPS = 1e-7\n",
        "\n",
        "train_loss = keras.losses.MeanSquaredError()\n",
        "\n",
        "### a simple MLP residual block contains:\n",
        "### 1. dense layer with ReLU as activation - lin_a\n",
        "### 2. dense linear layer - lin_b\n",
        "### 3. skip connection - lin_res\n",
        "class MLPResidual(keras.layers.Layer):\n",
        "  \"\"\"Simple one hidden state residual network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, hidden_dim, output_dim, layer_norm=False, dropout_rate=0.0\n",
        "  ):\n",
        "    super(MLPResidual, self).__init__()\n",
        "    self.lin_a = tf.keras.layers.Dense(\n",
        "        hidden_dim,\n",
        "        activation='relu',\n",
        "    )\n",
        "    self.lin_b = tf.keras.layers.Dense(\n",
        "        output_dim,\n",
        "        activation=None,\n",
        "    )\n",
        "    self.lin_res = tf.keras.layers.Dense(\n",
        "        output_dim,\n",
        "        activation=None,\n",
        "    )\n",
        "    if layer_norm:\n",
        "      self.lnorm = tf.keras.layers.LayerNormalization()\n",
        "    self.layer_norm = layer_norm\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"Call method.\"\"\"\n",
        "    h_state = self.lin_a(inputs)\n",
        "    out = self.lin_b(h_state)\n",
        "    out = self.dropout(out)\n",
        "    res = self.lin_res(inputs)\n",
        "    if self.layer_norm:\n",
        "      return self.lnorm(out + res)\n",
        "    return out + res\n",
        "\n",
        "### funciton to make multi layer dense encoder and dense decoder\n",
        "def _make_dnn_residual(hidden_dims, layer_norm=False, dropout_rate=0.0):\n",
        "  \"\"\"Multi-layer DNN residual model.\"\"\"\n",
        "  if len(hidden_dims) < 2:\n",
        "    return keras.layers.Dense(\n",
        "        hidden_dims[-1],\n",
        "        activation=None,\n",
        "    )\n",
        "  layers = []\n",
        "  for i, hdim in enumerate(hidden_dims[:-1]):\n",
        "    layers.append(\n",
        "        MLPResidual(\n",
        "            hdim,\n",
        "            hidden_dims[i + 1],\n",
        "            layer_norm=layer_norm,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "    )\n",
        "  return keras.Sequential(layers)\n",
        "\n",
        "\n",
        "class TideModel(keras.Model):\n",
        "  \"\"\"Main class for multi-scale DNN model.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_config,\n",
        "      pred_len,\n",
        "      cat_sizes,\n",
        "      num_ts,\n",
        "      transform=False,\n",
        "      cat_emb_size=4,\n",
        "      layer_norm=False,\n",
        "      dropout_rate=0.0,\n",
        "  ):\n",
        "    \"\"\"Tide model.\n",
        "\n",
        "    Args:\n",
        "      model_config: configurations specific to the model.\n",
        "      pred_len: prediction horizon length.\n",
        "      cat_sizes: number of categories in each categorical covariate.\n",
        "      num_ts: number of time-series in the dataset\n",
        "      transform: apply reversible transform or not.\n",
        "      cat_emb_size: embedding size of categorical variables.\n",
        "      layer_norm: use layer norm or not.\n",
        "      dropout_rate: level of dropout.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.model_config = model_config\n",
        "    self.transform = transform\n",
        "    if self.transform:\n",
        "      self.affine_weight = self.add_weight(\n",
        "          name='affine_weight',\n",
        "          shape=(num_ts,),\n",
        "          initializer='ones',\n",
        "          trainable=True,\n",
        "      )\n",
        "\n",
        "      self.affine_bias = self.add_weight(\n",
        "          name='affine_bias',\n",
        "          shape=(num_ts,),\n",
        "          initializer='zeros',\n",
        "          trainable=True,\n",
        "      )\n",
        "    self.pred_len = pred_len\n",
        "\n",
        "    ### dense encoder has multiple layers with the same hiddenSize\n",
        "    self.encoder = _make_dnn_residual(\n",
        "        model_config.get('hidden_dims'),\n",
        "        layer_norm=layer_norm,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "\n",
        "    ### the last layer of dense decoder has an output dim as H*p\n",
        "    self.decoder = _make_dnn_residual(\n",
        "        model_config.get('hidden_dims')[:-1]\n",
        "        + [\n",
        "            model_config.get('decoder_output_dim') * self.pred_len,\n",
        "        ],\n",
        "        layer_norm=layer_norm,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "    self.linear = tf.keras.layers.Dense(\n",
        "        self.pred_len,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "    ### time covariates are projected using a MLPResidual\n",
        "    self.time_encoder = _make_dnn_residual(\n",
        "        model_config.get('time_encoder_dims'),\n",
        "        layer_norm=layer_norm,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "\n",
        "    ### temporal decoder has an output dim as 1, to generate prediction for specific time-series\n",
        "    self.final_decoder = MLPResidual(\n",
        "        hidden_dim=model_config.get('final_decoder_hidden'),\n",
        "        output_dim=1,\n",
        "        layer_norm=layer_norm,\n",
        "        dropout_rate=dropout_rate,\n",
        "    )\n",
        "\n",
        "    ### catgorical embedding and time-series embedding\n",
        "    self.cat_embs = []\n",
        "    for cat_size in cat_sizes:\n",
        "      self.cat_embs.append(\n",
        "          tf.keras.layers.Embedding(input_dim=cat_size, output_dim=cat_emb_size)\n",
        "      )\n",
        "    self.ts_embs = tf.keras.layers.Embedding(input_dim=num_ts, output_dim=16)\n",
        "\n",
        "  @tf.function\n",
        "  def _assemble_feats(self, feats, cfeats):\n",
        "    \"\"\"assemble all features.\"\"\"\n",
        "    all_feats = [feats]\n",
        "    for i, emb in enumerate(self.cat_embs):\n",
        "      all_feats.append(tf.transpose(emb(cfeats[i, :])))\n",
        "    return tf.concat(all_feats, axis=0)\n",
        "\n",
        "  @tf.function\n",
        "  def call(self, inputs):\n",
        "    \"\"\"Call function that takes in a batch of training data and features.\"\"\"\n",
        "    ### inputs is \"all_data\" created by train_gen and eval_test_gen\n",
        "    ### [bts_train, bfeats_train, bcf_train, bts_pred, bfeats_pred, bcf_pred, tsidx]\n",
        "    ### past_data: time series within lookback period\n",
        "    ### past_ts: bts_train\n",
        "    ### past_feats: features including attribute, numerical and categorical covariates\n",
        "    ### future_feats: bfeats_pred + bcf_pred\n",
        "    past_data = inputs[0]\n",
        "    future_features = inputs[1]\n",
        "    bsize = past_data[0].shape[0]\n",
        "    tsidx = inputs[2]\n",
        "    past_feats = self._assemble_feats(past_data[1], past_data[2])\n",
        "    future_feats = self._assemble_feats(future_features[0], future_features[1])\n",
        "    past_ts = past_data[0]\n",
        "\n",
        "    ### batch normalization and affine transformation\n",
        "    if self.transform:\n",
        "      affine_weight = tf.gather(self.affine_weight, tsidx)\n",
        "      affine_bias = tf.gather(self.affine_bias, tsidx)\n",
        "      batch_mean = tf.math.reduce_mean(past_ts, axis=1)\n",
        "      batch_std = tf.math.reduce_std(past_ts, axis=1)\n",
        "      batch_std = tf.where(\n",
        "          tf.math.equal(batch_std, 0.0), tf.ones_like(batch_std), batch_std\n",
        "      )\n",
        "      past_ts = (past_ts - batch_mean[:, None]) / batch_std[:, None]\n",
        "      past_ts = affine_weight[:, None] * past_ts + affine_bias[:, None]\n",
        "\n",
        "    ### training flow\n",
        "    encoded_past_feats = tf.transpose(\n",
        "        self.time_encoder(tf.transpose(past_feats))\n",
        "    )\n",
        "    encoded_future_feats = tf.transpose(\n",
        "        self.time_encoder(tf.transpose(future_feats))\n",
        "    )\n",
        "    enc_past = tf.repeat(tf.expand_dims(encoded_past_feats, axis=0), bsize, 0)\n",
        "    enc_past = tf.reshape(enc_past, [bsize, -1])\n",
        "    enc_fut = tf.repeat(\n",
        "        tf.expand_dims(encoded_future_feats, axis=0), bsize, 0\n",
        "    )  # batch x fdim x H\n",
        "    enc_future = tf.reshape(enc_fut, [bsize, -1])\n",
        "    residual_out = self.linear(past_ts)\n",
        "    ts_embs = self.ts_embs(tsidx)\n",
        "    encoder_input = tf.concat([past_ts, enc_past, enc_future, ts_embs], axis=1)\n",
        "    encoding = self.encoder(encoder_input)\n",
        "    decoder_out = self.decoder(encoding)\n",
        "    decoder_out = tf.reshape(\n",
        "        decoder_out, [bsize, -1, self.pred_len]\n",
        "    )  # batch x d x H\n",
        "    final_in = tf.concat([decoder_out, enc_fut], axis=1)\n",
        "    out = self.final_decoder(tf.transpose(final_in, (0, 2, 1)))  # B x H x 1\n",
        "    out = tf.squeeze(out, axis=-1)\n",
        "    out += residual_out\n",
        "    if self.transform:\n",
        "      out = (out - affine_bias[:, None]) / (affine_weight[:, None] + EPS)\n",
        "      out = out * batch_std[:, None] + batch_mean[:, None]\n",
        "    return out\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, past_data, future_features, ytrue, tsidx, optimizer):\n",
        "    \"\"\"One step of training.\"\"\"\n",
        "\n",
        "    ### mathematic calculation of loss\n",
        "    with tf.GradientTape() as tape:\n",
        "      all_preds = self((past_data, future_features, tsidx), training=True)\n",
        "      loss = train_loss(ytrue, all_preds)\n",
        "\n",
        "    ### mathematic calculation of derivatives\n",
        "    grads = tape.gradient(loss, self.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "  def get_all_eval_data(self, data, mode, num_split=1):\n",
        "    y_preds = []\n",
        "    y_trues = []\n",
        "    all_test_loss = 0\n",
        "    all_test_num = 0\n",
        "    idxs = np.arange(0, self.pred_len, self.pred_len // num_split).tolist() + [\n",
        "        self.pred_len\n",
        "    ]\n",
        "    for i in range(len(idxs) - 1):\n",
        "      indices = (idxs[i], idxs[i + 1])\n",
        "      logging.info('Getting data for indices: %s', indices)\n",
        "      all_y_true, all_y_pred, test_loss, test_num = (\n",
        "          self.get_eval_data_for_split(data, mode, indices)\n",
        "      )\n",
        "      y_preds.append(all_y_pred)\n",
        "      y_trues.append(all_y_true)\n",
        "      all_test_loss += test_loss\n",
        "      all_test_num += test_num\n",
        "    return np.hstack(y_preds), np.hstack(y_trues), all_test_loss / all_test_num\n",
        "\n",
        "  def get_eval_data_for_split(self, data, mode, indices):\n",
        "    iterator = data.tf_dataset(mode=mode)\n",
        "\n",
        "    all_y_true = None\n",
        "    all_y_pred = None\n",
        "\n",
        "    def set_or_concat(a, b):\n",
        "      if a is None:\n",
        "        return b\n",
        "      return tf.concat((a, b), axis=1)\n",
        "\n",
        "    all_test_loss = 0\n",
        "    all_test_num = 0\n",
        "    ts_count = 0\n",
        "    ypreds = []\n",
        "    ytrues = []\n",
        "    for all_data in tqdm(iterator):\n",
        "      past_data = all_data[:3]\n",
        "      future_features = all_data[4:6]\n",
        "      y_true = all_data[3]\n",
        "      tsidx = all_data[-1]\n",
        "      all_preds = self((past_data, future_features, tsidx), training=False)\n",
        "      y_pred = all_preds\n",
        "      y_pred = y_pred[:, 0 : y_true.shape[1]]\n",
        "      id1 = indices[0]\n",
        "      id2 = min(indices[1], y_true.shape[1])\n",
        "      y_pred = y_pred[:, id1:id2]\n",
        "      y_true = y_true[:, id1:id2]\n",
        "      loss = train_loss(y_true, y_pred)\n",
        "      all_test_loss += loss\n",
        "      all_test_num += 1\n",
        "      ts_count += y_true.shape[0]\n",
        "      ypreds.append(y_pred)\n",
        "      ytrues.append(y_true)\n",
        "      if ts_count >= len(data.ts_cols):\n",
        "        ts_count = 0\n",
        "        ypreds = tf.concat(ypreds, axis=0)\n",
        "        ytrues = tf.concat(ytrues, axis=0)\n",
        "        all_y_true = set_or_concat(all_y_true, ytrues)\n",
        "        all_y_pred = set_or_concat(all_y_pred, ypreds)\n",
        "        ypreds = []\n",
        "        ytrues = []\n",
        "    return (\n",
        "        all_y_true.numpy(),\n",
        "        all_y_pred.numpy(),\n",
        "        all_test_loss.numpy(),\n",
        "        all_test_num,\n",
        "    )\n",
        "\n",
        "  def eval(self, data, mode, num_split=1):\n",
        "    all_y_pred, all_y_true, test_loss = self.get_all_eval_data(\n",
        "        data, mode, num_split\n",
        "    )\n",
        "\n",
        "    result_dict = {}\n",
        "    for metric in METRICS:\n",
        "      eval_fn = METRICS[metric]\n",
        "      result_dict[metric] = np.float64(eval_fn(all_y_pred, all_y_true))\n",
        "\n",
        "    logging.info(result_dict)\n",
        "    logging.info('Loss: %f', test_loss)\n",
        "\n",
        "    return (\n",
        "        result_dict,\n",
        "        (all_y_pred, all_y_true),\n",
        "        test_loss,\n",
        "    )\n",
        "\n",
        "\n",
        "def mape(y_pred, y_true):\n",
        "  abs_diff = np.abs(y_pred - y_true).flatten()\n",
        "  abs_val = np.abs(y_true).flatten()\n",
        "  idx = np.where(abs_val > EPS)\n",
        "  mpe = np.mean(abs_diff[idx] / abs_val[idx])\n",
        "  return mpe\n",
        "\n",
        "\n",
        "def mae_loss(y_pred, y_true):\n",
        "  return np.abs(y_pred - y_true).mean()\n",
        "\n",
        "\n",
        "def wape(y_pred, y_true):\n",
        "  abs_diff = np.abs(y_pred - y_true)\n",
        "  abs_val = np.abs(y_true)\n",
        "  wpe = np.sum(abs_diff) / (np.sum(abs_val) + EPS)\n",
        "  return wpe\n",
        "\n",
        "\n",
        "def smape(y_pred, y_true):\n",
        "  abs_diff = np.abs(y_pred - y_true)\n",
        "  abs_mean = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "  smpe = np.mean(abs_diff / (abs_mean + EPS))\n",
        "  return smpe\n",
        "\n",
        "\n",
        "def rmse(y_pred, y_true):\n",
        "  return np.sqrt(np.square(y_pred - y_true).mean())\n",
        "\n",
        "\n",
        "def nrmse(y_pred, y_true):\n",
        "  mse = np.square(y_pred - y_true)\n",
        "  return np.sqrt(mse.mean()) / np.abs(y_true).mean()\n",
        "\n",
        "\n",
        "METRICS = {\n",
        "    'mape': mape,\n",
        "    'wape': wape,\n",
        "    'smape': smape,\n",
        "    'nrmse': nrmse,\n",
        "    'rmse': rmse,\n",
        "    'mae': mae_loss,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzhD82GPUgL8"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQSJu9pT_A3X"
      },
      "source": [
        "## Reproduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTyXzZSE-ngS"
      },
      "source": [
        "We run the model on the electricity.csv dataset and part of the results is displayed in the markdown block at the end of this file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu3uEFK7XswJ"
      },
      "outputs": [],
      "source": [
        "FLAGS = flags.FLAGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTfc7ogmZgS7",
        "outputId": "f4ab3519-f4e2-4430-d56e-0de0b18f32af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<absl.flags._flagvalues.FlagHolder at 0x78d208c642e0>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.compat.v1.flags.DEFINE_string('f','','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngRzQUh23sX8"
      },
      "outputs": [],
      "source": [
        "flags.DEFINE_integer('train_epochs', 100, 'Number of epochs to train')\n",
        "flags.DEFINE_integer('patience', 40, 'Patience for early stopping')\n",
        "flags.DEFINE_integer('epoch_len', None, 'number of iterations in an epoch')\n",
        "flags.DEFINE_integer(\n",
        "    'batch_size', 512, 'Batch size for the randomly sampled batch'\n",
        ")\n",
        "flags.DEFINE_float('learning_rate', 1e-4, 'Learning rate')\n",
        "\n",
        "\n",
        "# Non tunable flags\n",
        "flags.DEFINE_string(\n",
        "    'expt_dir',\n",
        "    './results',\n",
        "    'The name of the experiment dir',\n",
        ")\n",
        "flags.DEFINE_string('dataset', 'elec', 'The name of the dataset.')\n",
        "flags.DEFINE_string('datetime_col', 'date', 'Column having datetime.')\n",
        "flags.DEFINE_list('num_cov_cols', None, 'Column having numerical features.')\n",
        "flags.DEFINE_list('cat_cov_cols', None, 'Column having categorical features.')\n",
        "flags.DEFINE_integer('hist_len', 720, 'Length of the history provided as input')\n",
        "flags.DEFINE_integer('pred_len', 720, 'Length of pred len during training')\n",
        "flags.DEFINE_integer('num_layers', 2, 'Number of DNN layers')\n",
        "flags.DEFINE_integer('hidden_size', 256, 'Hidden size of DNN')\n",
        "flags.DEFINE_integer('decoder_output_dim', 4, 'Hidden d3 of DNN')\n",
        "flags.DEFINE_integer('final_decoder_hidden', 64, 'Hidden d3 of DNN')\n",
        "flags.DEFINE_list('ts_cols', None, 'Columns of time-series features')\n",
        "flags.DEFINE_integer(\n",
        "    'random_seed', None, 'The random seed to be used for TF and numpy'\n",
        ")\n",
        "flags.DEFINE_bool('normalize', True, 'normalize data for training or not')\n",
        "flags.DEFINE_bool('holiday', False, 'use holiday features or not')\n",
        "flags.DEFINE_bool('permute', True, 'permute the order of TS in training set')\n",
        "flags.DEFINE_bool('transform', False, 'Apply chronoml transform or not.')\n",
        "flags.DEFINE_bool('layer_norm', False, 'Apply layer norm or not.')\n",
        "flags.DEFINE_float('dropout_rate', 0.0, 'dropout rate')\n",
        "flags.DEFINE_integer('num_split', 1, 'number of splits during inference.')\n",
        "flags.DEFINE_integer(\n",
        "    'min_num_epochs', 0, 'minimum number of epochs before early stopping'\n",
        ")\n",
        "flags.DEFINE_integer('gpu', 0, 'index of gpu to be used.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp87vFclX-mH"
      },
      "outputs": [],
      "source": [
        "\"\"\"Main training code.\"\"\"\n",
        "\n",
        "### run model on the elec dataset.\n",
        "\n",
        "DATA_DICT = {\n",
        "    # 'ettm2': {\n",
        "    #     'boundaries': [34560, 46080, 57600],\n",
        "    #     'data_path': './datasets/ETT-small/ETTm2.csv',\n",
        "    #     'freq': '15min',\n",
        "    # },\n",
        "    # 'ettm1': {\n",
        "    #     'boundaries': [34560, 46080, 57600],\n",
        "    #     'data_path': './datasets/ETT-small/ETTm1.csv',\n",
        "    #     'freq': '15min',\n",
        "    # },\n",
        "    # 'etth2': {\n",
        "    #     'boundaries': [8640, 11520, 14400],\n",
        "    #     'data_path': './datasets/ETT-small/ETTh2.csv',\n",
        "    #     'freq': 'H',\n",
        "    # },\n",
        "    # 'etth1': {\n",
        "    #     'boundaries': [8640, 11520, 14400],\n",
        "    #     'data_path': './datasets/ETT-small/ETTh1.csv',\n",
        "    #     'freq': 'H',\n",
        "    # },\n",
        "    'elec': {\n",
        "        'boundaries': [18413, 21044, 26304],\n",
        "        'data_path': './datasets/electricity/electricity.csv',\n",
        "        'freq': 'H',\n",
        "    },\n",
        "    # 'traffic': {\n",
        "    #     'boundaries': [12280, 14036, 17544],\n",
        "    #     'data_path': './datasets/traffic/traffic.csv',\n",
        "    #     'freq': 'H',\n",
        "    # },\n",
        "    # 'weather': {\n",
        "    #     'boundaries': [36887, 42157, 52696],\n",
        "    #     'data_path': './datasets/weather/weather.csv',\n",
        "    #     'freq': '10min',\n",
        "    # },\n",
        "}\n",
        "\n",
        "np.random.seed(1024)\n",
        "tf.random.set_seed(1024)\n",
        "\n",
        "\n",
        "def _get_random_string(num_chars):\n",
        "  rand_str = ''.join(\n",
        "      random.choice(\n",
        "          string.ascii_uppercase + string.ascii_lowercase + string.digits\n",
        "      )\n",
        "      for _ in range(num_chars - 1)\n",
        "  )\n",
        "  return rand_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qRT5nLQUgL9"
      },
      "outputs": [],
      "source": [
        "def training():\n",
        "  \"\"\"Training TS code.\"\"\"\n",
        "  tf.random.set_seed(FLAGS.random_seed)\n",
        "  np.random.seed(FLAGS.random_seed)\n",
        "\n",
        "  experiment_id = _get_random_string(8)\n",
        "  logging.info('Experiment id: %s', experiment_id)\n",
        "\n",
        "  dataset = FLAGS.dataset\n",
        "  data_path = DATA_DICT[dataset]['data_path']\n",
        "  freq = DATA_DICT[dataset]['freq']\n",
        "  boundaries = DATA_DICT[dataset]['boundaries']\n",
        "\n",
        "  data_df = pd.read_csv(open(data_path, 'r'))\n",
        "\n",
        "  if FLAGS.ts_cols:\n",
        "    ts_cols = DATA_DICT[dataset]['ts_cols']\n",
        "    num_cov_cols = DATA_DICT[dataset]['num_cov_cols']\n",
        "    cat_cov_cols = DATA_DICT[dataset]['cat_cov_cols']\n",
        "  else:\n",
        "    ts_cols = [col for col in data_df.columns if col != FLAGS.datetime_col]\n",
        "    num_cov_cols = None\n",
        "    cat_cov_cols = None\n",
        "  permute = FLAGS.permute\n",
        "  dtl = TimeSeriesdata(\n",
        "      data_path=data_path,\n",
        "      datetime_col=FLAGS.datetime_col,\n",
        "      num_cov_cols=num_cov_cols,\n",
        "      cat_cov_cols=cat_cov_cols,\n",
        "      ts_cols=np.array(ts_cols),\n",
        "      train_range=[0, boundaries[0]],\n",
        "      val_range=[boundaries[0], boundaries[1]],\n",
        "      test_range=[boundaries[1], boundaries[2]],\n",
        "      hist_len=FLAGS.hist_len,\n",
        "      pred_len=FLAGS.pred_len,\n",
        "      batch_size=min(FLAGS.batch_size, len(ts_cols)),\n",
        "      freq=freq,\n",
        "      normalize=FLAGS.normalize,\n",
        "      epoch_len=FLAGS.epoch_len,\n",
        "      holiday=FLAGS.holiday,\n",
        "      permute=permute,\n",
        "  )\n",
        "\n",
        "  # Create model\n",
        "  model_config = {\n",
        "      'model_type': 'dnn',\n",
        "      'hidden_dims': [FLAGS.hidden_size] * FLAGS.num_layers,\n",
        "      'time_encoder_dims': [64, 4],\n",
        "      'decoder_output_dim': FLAGS.decoder_output_dim,\n",
        "      'final_decoder_hidden': FLAGS.final_decoder_hidden,\n",
        "      'batch_size': dtl.batch_size,\n",
        "  }\n",
        "  model = TideModel(\n",
        "      model_config=model_config,\n",
        "      pred_len=FLAGS.pred_len,\n",
        "      num_ts=len(ts_cols),\n",
        "      cat_sizes=dtl.cat_sizes,\n",
        "      transform=FLAGS.transform,\n",
        "      layer_norm=FLAGS.layer_norm,\n",
        "      dropout_rate=FLAGS.dropout_rate,\n",
        "  )\n",
        "\n",
        "  # Compute path to experiment directory\n",
        "  expt_dir = os.path.join(\n",
        "      FLAGS.expt_dir,\n",
        "      FLAGS.dataset + '_' + str(experiment_id) + '_' + str(FLAGS.pred_len),\n",
        "  )\n",
        "  os.makedirs(expt_dir, exist_ok=True)\n",
        "\n",
        "  step = tf.Variable(0)\n",
        "  # LR scheduling\n",
        "  lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
        "      initial_learning_rate=FLAGS.learning_rate,\n",
        "      decay_steps=30 * dtl.train_range[1],\n",
        "  )\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=lr_schedule, clipvalue=1e3)\n",
        "  summary = Summary(expt_dir)\n",
        "\n",
        "  best_loss = np.inf\n",
        "  pat = 0\n",
        "  mean_loss_array = []\n",
        "  iter_array = []\n",
        "  # best_check_path = None\n",
        "  while step.numpy() < FLAGS.train_epochs + 1:\n",
        "    ep = step.numpy()\n",
        "    logging.info('Epoch %s', ep)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    iterator = tqdm(dtl.tf_dataset(mode='train'), mininterval=2)\n",
        "    for i, batch in enumerate(iterator):\n",
        "      past_data = batch[:3]\n",
        "      future_features = batch[4:6]\n",
        "      tsidx = batch[-1]\n",
        "      loss = model.train_step(\n",
        "          past_data, future_features, batch[3], tsidx, optimizer\n",
        "      )\n",
        "      # Train metrics\n",
        "      summary.update({'train/reg_loss': loss, 'train/loss': loss})\n",
        "      if i % 100 == 0:\n",
        "        mean_loss = summary.metric_dict['train/reg_loss'].result().numpy()\n",
        "        mean_loss_array.append(mean_loss)\n",
        "        iter_array.append(i)\n",
        "        iterator.set_description(f'Loss {mean_loss:.4f}')\n",
        "    step.assign_add(1)\n",
        "    # Test metrics\n",
        "    val_metrics, val_res, val_loss = model.eval(\n",
        "        dtl, 'val', num_split=FLAGS.num_split\n",
        "    )\n",
        "    test_metrics, test_res, test_loss = model.eval(\n",
        "        dtl, 'test', num_split=FLAGS.num_split\n",
        "    )\n",
        "    logging.info('Val Loss: %s', val_loss)\n",
        "    logging.info('Test Loss: %s', test_loss)\n",
        "    tracked_loss = val_metrics['rmse']\n",
        "    if tracked_loss < best_loss and ep > FLAGS.min_num_epochs:\n",
        "      best_loss = tracked_loss\n",
        "      pat = 0\n",
        "\n",
        "      with open(os.path.join(expt_dir, 'val_pred.npy'), 'wb') as fp:\n",
        "        np.save(fp, val_res[0][:, 0 : -1 : FLAGS.pred_len])\n",
        "      with open(os.path.join(expt_dir, 'val_true.npy'), 'wb') as fp:\n",
        "        np.save(fp, val_res[1][:, 0 : -1 : FLAGS.pred_len])\n",
        "\n",
        "      with open(os.path.join(expt_dir, 'test_pred.npy'), 'wb') as fp:\n",
        "        np.save(fp, test_res[0][:, 0 : -1 : FLAGS.pred_len])\n",
        "      with open(os.path.join(expt_dir, 'test_true.npy'), 'wb') as fp:\n",
        "        np.save(fp, test_res[1][:, 0 : -1 : FLAGS.pred_len])\n",
        "      with open(os.path.join(expt_dir, 'test_metrics.json'), 'w') as fp:\n",
        "        json.dump(test_metrics, fp)\n",
        "      logging.info('saved best result so far at %s', expt_dir)\n",
        "      logging.info('Test metrics: %s', test_metrics)\n",
        "    else:\n",
        "      pat += 1\n",
        "      if pat > FLAGS.patience:\n",
        "        logging.info('Early stopping')\n",
        "        break\n",
        "    summary.write(step=step.numpy())\n",
        "\n",
        "\n",
        "class Summary:\n",
        "  \"\"\"Summary statistics.\"\"\"\n",
        "\n",
        "  def __init__(self, log_dir):\n",
        "    self.metric_dict = {}\n",
        "    self.writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "  def update(self, update_dict):\n",
        "    for metric in update_dict:\n",
        "      if metric not in self.metric_dict:\n",
        "        self.metric_dict[metric] = keras.metrics.Mean()\n",
        "      self.metric_dict[metric].update_state(values=[update_dict[metric]])\n",
        "\n",
        "  def write(self, step):\n",
        "    with self.writer.as_default():\n",
        "      for metric in self.metric_dict:\n",
        "        tf.summary.scalar(metric, self.metric_dict[metric].result(), step=step)\n",
        "    self.metric_dict = {}\n",
        "    self.writer.flush()\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VGDXTyJYmX8"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  app.run(main)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3-QOCn3_j0H"
      },
      "source": [
        "## Part of the reproducing results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9sjB71O_qv1"
      },
      "source": [
        "Due to the limited resources (i.e GPU), the reproducing results we obtained are listed below.\n",
        "\n",
        "Within 22 hours, our device is only able to run 2 epoches for the first dataset (from 10am to 8am). We also try to run on google colab, though the running speed is higher (48it/s vs 9it/s on local environment), while personal trial of google account was occupied after running 2 epochs for electricity dataset. We suggest higher capacity of device when reproducing codes, such as cloud platform with higher capacity or at least i7/i10 core for higher running speed. Due to time limits, we are not able to manage to complete all reproduction, but it is capable with recommendations above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94tUgyNd_XZV"
      },
      "source": [
        "I1102 13:46:49.093221 140704456412800 train.py:146] Experiment id: XOZsbFA\n",
        "\n",
        "I1102 13:46:51.977347 140704456412800 data_loader.py:116] Data Shapes: (321, 26304), (7, 27025), (1, 26304), (1, 26304)\n",
        "\n",
        "2023-11-02 13:46:52.003338: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
        "\n",
        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "\n",
        "**I1102 13:46:52.125188 140704456412800 train.py:226] Epoch 0**\n",
        "\n",
        "0it [00:00, ?it/s]I1102 13:46:52.394349 123145383587840 data_loader.py:152] Hist len: 720\n",
        "\n",
        "Loss 0.2096: : 33946it [1:50:31,  5.12it/s]\n",
        "\n",
        "I1102 15:37:23.699673 140704456412800 models.py:242] Getting data for indices: (0, 720)\n",
        "\n",
        "0it [00:00, ?it/s]I1102 15:37:24.198838 123145411645440 data_loader.py:196] Hist len: 720\n",
        "\n",
        "1912it [16:34,  1.92it/s]\n",
        "\n",
        "I1102 15:55:51.894650 140704456412800 models.py:311] {'mape': 2.3208236694335938, 'wape': 0.3506520078911239, 'smape': 0.5850090384483337, 'nrmse': 0.5434972643852234, 'rmse': 0.40702003240585327, 'mae': 0.26260003447532654}\n",
        "\n",
        "I1102 15:55:51.915013 140704456412800 models.py:312] Loss: 0.165665\n",
        "\n",
        "I1102 15:55:52.010943 140704456412800 models.py:242] Getting data for indices: (0, 720)\n",
        "\n",
        "0it [00:00, ?it/s]I1102 15:55:52.620486 123145411645440 data_loader.py:196] Hist len: 720\n",
        "\n",
        "4541it [4:52:30,  3.86s/it]\n",
        "\n",
        "I1102 21:12:12.041450 140704456412800 models.py:311] {'mape': 2.3315513134002686, 'wape': 0.3833434787576122, 'smape': 0.5768746733665466, 'nrmse': 0.5675468444824219, 'rmse': 0.47104641795158386, 'mae': 0.31816330552101135}\n",
        "\n",
        "I1102 21:12:12.131451 140704456412800 models.py:312] Loss: 0.221885\n",
        "\n",
        "I1102 21:12:12.141915 140704456412800 train.py:252] Val Loss: 0.16566508783954956\n",
        "\n",
        "I1102 21:12:12.146863 140704456412800 train.py:253] Test Loss: 0.22188502422804035\n",
        "\n",
        "**I1102 21:12:13.022655 140704456412800 train.py:226] Epoch 1**\n",
        "\n",
        "0it [00:00, ?it/s]I1102 21:12:13.991549 123145411645440 data_loader.py:152] Hist len: 720\n",
        "\n",
        "Loss 0.1617: : 33946it [1:42:52,  5.50it/s]\n",
        "\n",
        "I1102 22:55:06.594316 140704456412800 models.py:242] Getting data for indices: (0, 720)\n",
        "\n",
        "0it [00:00, ?it/s]I1102 22:55:06.772835 123145411645440 data_loader.py:196] Hist len: 720\n",
        "\n",
        "1912it [14:52,  2.14it/s]\n",
        "\n",
        "I1102 23:11:41.612770 140704456412800 models.py:311] {'mape': 2.183418035507202, 'wape': 0.3475286474837033, 'smape': 0.5901986956596375, 'nrmse': 0.5428066253662109, 'rmse': 0.4065028429031372, 'mae': 0.2602609694004059}\n",
        "\n",
        "I1102 23:11:41.622642 140704456412800 models.py:312] Loss: 0.165243\n",
        "\n",
        "I1102 23:11:41.681569 140704456412800 models.py:242] Getting data for indices: (0, 720)\n",
        "\n",
        "0it [00:00, ?it/s]I1102 23:11:42.006523 123145384124416 data_loader.py:196] Hist len: 720\n",
        "\n",
        "4541it [3:20:40,  2.65s/it]\n",
        "\n",
        "I1103 02:52:24.920064 140704456412800 models.py:311] {'mape': 2.221997022628784, 'wape': 0.37962246531238414, 'smape': 0.5742937326431274, 'nrmse': 0.5680287480354309, 'rmse': 0.4714463949203491, 'mae': 0.31507495045661926}\n",
        "\n",
        "I1103 02:52:24.989138 140704456412800 models.py:312] Loss: 0.222263\n",
        "\n",
        "I1103 02:52:25.181510 140704456412800 train.py:252] Val Loss: 0.1652434440836248\n",
        "\n",
        "I1103 02:52:25.182302 140704456412800 train.py:253] Test Loss: 0.22226341262679614\n",
        "\n",
        "I1103 02:53:28.397828 140704456412800 train.py:270] saved best result so far at ./results/elec_XOZsbFA_720\n",
        "\n",
        "I1103 02:53:28.414479 140704456412800 train.py:271] Test metrics: {'mape': 2.221997022628784, 'wape': 0.37962246531238414, 'smape': 0.5742937326431274, 'nrmse': 0.5680287480354309, 'rmse': 0.4714463949203491, 'mae': 0.31507495045661926}\n",
        "\n",
        "**I1103 02:53:29.736258 140704456412800 train.py:226] Epoch 2**\n",
        "\n",
        "0it [00:00, ?it/s]I1103 02:53:34.689530 123145384660992 data_loader.py:152] Hist len: 720\n",
        "\n",
        "Loss 0.1463: : 33946it [1:35:41,  5.91it/s]\n",
        "\n",
        "I1103 04:29:14.555700 140704456412800 models.py:242] Getting data for indices: (0, 720)\n",
        "\n",
        "0it [00:00, ?it/s]I1103 04:29:14.668433 123145412182016 data_loader.py:196] Hist len: 720\n",
        "\n",
        "1912it [14:18,  2.23it/s]\n",
        "\n",
        "I1103 04:44:44.205847 140704456412800 models.py:311] {'mape': 2.2145016193389893, 'wape': 0.3410096805703794, 'smape': 0.5751430988311768, 'nrmse': 0.5402857661247253, 'rmse': 0.40461498498916626, 'mae': 0.25537896156311035}\n",
        "\n",
        "I1103 04:44:44.221186 140704456412800 models.py:312] Loss: 0.163713\n",
        "\n",
        "I1103 04:44:44.273549 140704456412800 models.py:242] Getting data for indices: (0, 720)\n",
        "\n",
        "0it [00:00, ?it/s]I1103 04:44:44.733463 123145412182016 data_loader.py:196] Hist len: 720\n",
        "\n",
        "4541it [2:55:21,  2.32s/it]\n",
        "\n",
        "I1103 07:58:07.520910 140704456412800 models.py:311] {'mape': 2.151806354522705, 'wape': 0.3721396975328801, 'smape': 0.5671623349189758, 'nrmse': 0.5736586451530457, 'rmse': 0.4761190414428711, 'mae': 0.3088645040988922}\n",
        "\n",
        "I1103 07:58:07.595725 140704456412800 models.py:312] Loss: 0.226688\n",
        "\n",
        "I1103 07:58:07.730432 140704456412800 train.py:252] Val Loss: 0.16371311203705216\n",
        "\n",
        "I1103 07:58:07.730627 140704456412800 train.py:253] Test Loss: 0.22668814559361924\n",
        "\n",
        "I1103 07:59:01.703009 140704456412800 train.py:270] saved best result so far at ./results/elec_XOZsbFA_720\n",
        "\n",
        "I1103 07:59:01.706350 140704456412800 train.py:271] Test metrics: {'mape': 2.151806354522705, 'wape': 0.3721396975328801, 'smape': 0.5671623349189758, 'nrmse': 0.5736586451530457, 'rmse': 0.4761190414428711, 'mae': 0.3088645040988922}\n",
        "\n",
        "**I1103 07:59:02.294898 140704456412800 train.py:226] Epoch 3**\n",
        "\n",
        "0it [00:00, ?it/s]I1103 07:59:04.099279 123145411645440 data_loader.py:152] Hist len: 720\n",
        "\n",
        "Loss 0.1358: : 33946it [1:44:32,  5.41it/s]\n",
        "\n",
        "I1103 09:43:36.007414 140704456412800 models.py:242] Getting data for indices: (0, 720)\n",
        "\n",
        "0it [00:00, ?it/s]I1103 09:43:36.652187 123145412182016 data_loader.py:196] Hist len: 720\n",
        "\n",
        "1912it [16:35,  1.92it/s]\n",
        "\n",
        "I1103 10:02:00.228435 140704456412800 models.py:311] {'mape': 2.299938678741455, 'wape': 0.34981897640881904, 'smape': 0.5845889449119568, 'nrmse': 0.5494011044502258, 'rmse': 0.4114413857460022, 'mae': 0.2619761824607849}\n",
        "\n",
        "I1103 10:02:00.242700 140704456412800 models.py:312] Loss: 0.169284\n",
        "\n",
        "I1103 10:02:00.302606 140704456412800 models.py:242] Getting data for indices: (0, 720)\n",
        "\n",
        "0it [00:00, ?it/s]I1103 10:02:00.949230 123145384660992 data_loader.py:196] Hist len: 720\n",
        "     \n",
        "3573it [58:44, 12.79s/it]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The authors perform demand forecasting on m5 forecasting dataset. However, no py file wrt this experiment is provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ablation Study modified code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The authors perform ablation Study on a modified electricity dataset. We create the below code to modify the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### modification code for dataset\n",
        "\n",
        "data_df = pd.read_csv('datasets/electricity/electricity.csv')\n",
        "ts_cols = [col for col in data_df.columns if col != FLAGS.datetime_col]\n",
        "num_ts = len(ts_cols)\n",
        "ts_length = data_df.shape[0]\n",
        "\n",
        "### create event type a and b\n",
        "event_type_a = np.random.choice([0, 1], num_ts, p=[0.2, 0.8])\n",
        "event_type_b = np.random.choice([0, 1], num_ts, p=[0.2, 0.8])\n",
        "\n",
        "### statistical characteristic for event a\n",
        "mean_a = [1.0, 2.0, 2.0, 1.0]\n",
        "variance_a = 0.1\n",
        "event_type_a_covariates = np.random.normal(mean_a, np.sqrt(variance_a), size=(num_ts, 4))\n",
        "\n",
        "### statistical characteristic for event b\n",
        "### no detail information is provided by the authors, we make up one\n",
        "mean_b = [3.0, 2.5, 1.5, 2.0]\n",
        "variance_b = 0.15\n",
        "event_type_b_covariates = np.random.normal(mean_b, np.sqrt(variance_b), size=(num_ts, 4))\n",
        "\n",
        "### events occur for 24 contiguous hours\n",
        "for i in range(num_ts):\n",
        "    if event_type_a[i] == 1:\n",
        "        factor = np.random.uniform(3, 3.2)\n",
        "        data_df[i, :24] *= factor\n",
        "    if event_type_b[i] == 1:\n",
        "        factor = np.random.uniform(2, 2.2)\n",
        "        data_df[i, :24] /= factor\n",
        "\n",
        "modified_elec = data_df\n",
        "modified_elec.to_csv('datasets/electricity/modified_elec.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training flow section of the class TideModel should be modified as follow.  We do not implement as computing resoures are limited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TideModel(keras.Model):\n",
        "\n",
        "### ...\n",
        "### training flow\n",
        "    encoded_past_feats = tf.transpose(\n",
        "        self.time_encoder(tf.transpose(past_feats))\n",
        "    )\n",
        "    encoded_future_feats = tf.transpose(\n",
        "        self.time_encoder(tf.transpose(future_feats))\n",
        "    )\n",
        "    enc_past = tf.repeat(tf.expand_dims(encoded_past_feats, axis=0), bsize, 0)\n",
        "    enc_past = tf.reshape(enc_past, [bsize, -1])\n",
        "    enc_fut = tf.repeat(\n",
        "        tf.expand_dims(encoded_future_feats, axis=0), bsize, 0\n",
        "    )  # batch x fdim x H\n",
        "    enc_future = tf.reshape(enc_fut, [bsize, -1])\n",
        "    residual_out = self.linear(past_ts)\n",
        "    ts_embs = self.ts_embs(tsidx)\n",
        "    encoder_input = tf.concat([past_ts, enc_past, enc_future, ts_embs], axis=1)\n",
        "    encoding = self.encoder(encoder_input)\n",
        "    decoder_out = self.decoder(encoding)\n",
        "    decoder_out = tf.reshape(\n",
        "        decoder_out, [bsize, -1, self.pred_len]\n",
        "    )  # batch x d x H\n",
        "    \n",
        "    ###### deactivate the lines below\n",
        "    ###### final_in = tf.concat([decoder_out, enc_fut], axis=1)\n",
        "    ###### out = self.final_decoder(tf.transpose(final_in, (0, 2, 1)))  # B x H x 1\n",
        "    \n",
        "    ###### modified line below\n",
        "    ###### original line:\n",
        "    ###### out = tf.squeeze(out, axis=-1)\n",
        "    out = tf.squeeze(decoder_out, axis=-1)\n",
        "\n",
        "    out += residual_out\n",
        "    if self.transform:\n",
        "      out = (out - affine_bias[:, None]) / (affine_weight[:, None] + EPS)\n",
        "      out = out * batch_std[:, None] + batch_mean[:, None]\n",
        "    return out\n",
        "\n",
        "### ..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
